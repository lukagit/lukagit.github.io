<!DOCTYPE html>
<html>
<body>
<h1>Hello World</h1>
<p>I'm hosted with GitHub Pages! Let's roll. </p>

<p>
--------------------------------------------------------------------------
</p>

<h2>Code1</h2>
 <h2>
 
 DO YOU THINK I’M INSANE?”
This question came from Elon Musk near the very end of a long dinner we shared at a high-end
seafood restaurant in Silicon Valley. I’d gotten to the restaurant first and settled down with a gin and
tonic, knowing Musk would—as ever—be late. After about fifteen minutes, Musk showed up wearing
leather shoes, designer jeans, and a plaid dress shirt. Musk stands six foot one but ask anyone who
knows him and they’ll confirm that he seems much bigger than that. He’s absurdly broad-shouldered,
sturdy, and thick. You’d figure he would use this frame to his advantage and perform an alpha-male
strut when entering a room. Instead, he tends to be almost sheepish. It’s head tilted slightly down
while walking, a quick handshake hello after reaching the table, and then butt in seat. From there,
Musk needs a few minutes before he warms up and looks at ease.
Musk asked me to dinner for a negotiation of sorts. Eighteen months earlier, I’d informed him of
my plans to write a book about him, and he’d informed me of his plans not to cooperate. His rejection
stung but thrust me into dogged reporter mode. If I had to do this book without him, so be it. Plenty of
people had left Musk’s companies, Tesla Motors and SpaceX, and would talk, and I already knew a
lot of his friends. The interviews followed one after another, month after month, and two hundred or
so people into the process, I heard from Musk once again. He called me at home and declared that
things could go one of two ways: he could make my life very difficult or he could help with the
project after all. He’d be willing to cooperate if he could read the book before it went to publication,
and could add footnotes throughout it. He would not meddle with my text, but he wanted the chance to
set the record straight in spots that he deemed factually inaccurate. I understood where this was
coming from. Musk wanted a measure of control over his life’s story. He’s also wired like a scientist
and suffers mental anguish at the sight of a factual error. A mistake on a printed page would gnaw at
his soul—forever. While I could understand his perspective, I could not let him read the book, for
professional, personal, and practical reasons. Musk has his version of the truth, and it’s not always
the version of the truth that the rest of the world shares. He’s prone to verbose answers to even the
simplest of questions as well, and the thought of thirty-page footnotes seemed all too real. Still, we
agreed to have dinner, chat all this out, and see where it left us.
Our conversation began with a discussion of public-relations people. Musk burns through PR
staffers notoriously fast, and Tesla was in the process of hunting for a new communications chief.
“Who is the best PR person in the world?” he asked in a very Muskian fashion. Then we talked about
mutual acquaintances, Howard Hughes, and the Tesla factory. When the waiter stopped by to take our
order, Musk asked for suggestions that would work with his low-carb diet. He settled on chunks of
fried lobster soaked in black squid ink. The negotiation hadn’t begun, and Musk was already dishing.
He opened up about the major fear keeping him up at night: namely that Google’s cofounder and CEO
Larry Page might well have been building a fleet of artificial-intelligence-enhanced robots capable of
destroying mankind. “I’m really worried about this,” Musk said. It didn’t make Musk feel any better
that he and Page were very close friends and that he felt Page was fundamentally a well-intentioned
person and not Dr. Evil. In fact, that was sort of the problem. Page’s nice-guy nature left him assuming
that the machines would forever do our bidding. “I’m not as optimistic,” Musk said. “He could
produce something evil by accident.” As the food arrived, Musk consumed it. That is, he didn’t eat it
as much as he made it disappear rapidly with a few gargantuan bites. Desperate to keep Musk happy
and chatting, I handed him a big chunk of steak from my plate. The plan worked . . . for all of ninety
seconds. Meat. Hunk. Gone.
It took awhile to get Musk off the artificial intelligence doom-and-gloom talk and to the subject at
hand. Then, as we drifted toward the book, Musk started to feel me out, probing exactly why it was
that I wanted to write about him and calculating my intentions. When the moment presented itself, I
moved in and seized the conversation. Some adrenaline released and mixed with the gin, and I
launched into what was meant to be a forty-five-minute sermon about all the reasons Musk should let
me burrow deep into his life and do so while getting exactly none of the controls he wanted in return.
The speech revolved around the inherent limitations of footnotes, Musk coming off like a control
freak and my journalistic integrity being compromised. To my great surprise, Musk cut me off after a
couple of minutes and simply said, “Okay.” One thing that Musk holds in the highest regard is resolve,
and he respects people who continue on after being told no. Dozens of other journalists had asked him
to help with a book before, but I’d been the only annoying asshole who continued on after Musk’s
initial rejection, and he seemed to like that.
The dinner wound down with pleasant conversation and Musk laying waste to the low-carb diet.
A waiter showed up with a giant yellow cotton candy desert sculpture, and Musk dug into it, ripping
off handfuls of the sugary fluff. It was settled. Musk granted me access to the executives at his
companies, his friends, and his family. He would meet me for dinner once a month for as long as it
took. For the first time, Musk would let a reporter see the inner workings of his world. Two and a half
hours after we started, Musk put his hands on the table, made a move to get up, and then paused,
locked eyes with me, and busted out that incredible question: “Do you think I’m insane?” The oddity
of the moment left me speechless for a beat, while my every synapse fired trying to figure out if this
was some sort of riddle, and, if so, how it should be answered artfully. It was only after I’d spent lots
of time with Musk that I realized the question was more for him than me. Nothing I said would have
mattered. Musk was stopping one last time and wondering aloud if I could be trusted and then looking
into my eyes to make his judgment. A split second later, we shook hands and Musk drove off in a red
Tesla Model S sedan.
ANY STUDY OF ELON MUSK must begin at the headquarters of SpaceX, in Hawthorne, California
—a suburb of Los Angeles located a few miles from Los Angeles International Airport. It’s there that
visitors will find two giant posters of Mars hanging side by side on the wall leading up to Musk’s
cubicle. The poster to the left depicts Mars as it is today—a cold, barren red orb. The poster on the
right shows a Mars with a humongous green landmass surrounded by oceans. The planet has been
heated up and transformed to suit humans. Musk fully intends to try and make this happen. Turning
humans into space colonizers is his stated life’s purpose. “I would like to die thinking that humanity
has a bright future,” he said. “If we can solve sustainable energy and be well on our way to becoming
a multiplanetary species with a self-sustaining civilization on another planet—to cope with a worstcase scenario happening and extinguishing human consciousness—then,” and here he paused for a
moment, “I think that would be really good.”
If some of the things that Musk says and does sound absurd, that’s because on one level they very
much are. On this occasion, for example, Musk’s assistant had just handed him some cookies-andcream ice cream with sprinkles on top, and he then talked earnestly about saving humanity while a
blotch of the dessert hung from his lower lip.
Musk’s ready willingness to tackle impossible things has turned him into a deity in Silicon Valley,
where fellow CEOs like Page speak of him in reverential awe, and budding entrepreneurs strive “to
be like Elon” just as they had been striving in years past to mimic Steve Jobs. Silicon Valley, though,
operates within a warped version of reality, and outside the confines of its shared fantasy, Musk often
comes off as a much more polarizing figure. He’s the guy with the electric cars, solar panels, and
rockets peddling false hope. Forget Steve Jobs. Musk is a sci-fi version of P. T. Barnum who has
gotten extraordinarily rich by preying on people’s fear and self-hatred. Buy a Tesla. Forget about the
mess you’ve made of the planet for a while.
I’d long been a subscriber to this latter camp. Musk had struck me as a well-intentioned dreamer
—a card-carrying member of Silicon Valley’s techno-utopian club. This group tends to be a mix of
Ayn Rand devotees and engineer absolutists who see their hyperlogical worldviews as the Answer
for everyone. If we’d just get out of their way, they’d fix all our problems. One day, soon enough,
we’ll be able to download our brains to a computer, relax, and let their algorithms take care of
everything. Much of their ambition proves inspiring and their works helpful. But the techno-utopians
do get tiresome with their platitudes and their ability to prattle on for hours without saying much of
substance. More disconcerting is their underlying message that humans are flawed and our humanity is
an annoying burden that needs to be dealt with in due course. When I’d caught Musk at Silicon Valley
events, his highfalutin talk often sounded straight out of the techno-utopian playbook. And, most
annoyingly, his world-saving companies didn’t even seem to be doing all that well.
Yet, in the early part of 2012, the cynics like me had to take notice of what Musk was actually
accomplishing. His once-beleaguered companies were succeeding at unprecedented things. SpaceX
flew a supply capsule to the International Space Station and brought it safely back to Earth. Tesla
Motors delivered the Model S, a beautiful, all-electric sedan that took the automotive industry’s
breath away and slapped Detroit sober. These two feats elevated Musk to the rarest heights among
business titans. Only Steve Jobs could claim similar achievements in two such different industries,
sometimes putting out a new Apple product and a blockbuster Pixar movie in the same year. And yet,
Musk was not done. He was also the chairman and largest shareholder of SolarCity, a booming solar
energy company poised to file for an initial public offering. Musk had somehow delivered the biggest
advances the space, automotive, and energy industries had seen in decades in what felt like one fell
swoop.
It was in 2012 that I decided to see what Musk was like firsthand and to write a cover story about
him for Bloomberg Businessweek. At this point in Musk’s life, everything ran through his
assistant/loyal appendage Mary Beth Brown. She invited me to visit what I’ve come to refer to as
Musk Land.
Anyone arriving at Musk Land for the first time will have the same head-scratching experience.
You’re told to park at One Rocket Road in Hawthorne, where SpaceX has its HQ. It seems
impossible that anything good could call Hawthorne home. It’s a bleak part of Los Angeles County in
which groupings of rundown houses, run-down shops, and run-down eateries surround huge,
industrial complexes that appear to have been built during some kind of architectural Boring
Rectangle movement. Did Elon Musk really stick his company in the middle of this dreck? Then,
okay, things start to make more sense when you see one 550,000-square-foot rectangle painted an
ostentatious hue of “Unity of Body, Soul, and Mind” white. This is the main SpaceX building.
It was only after going through the front doors of SpaceX that the grandeur of what this man had
done became apparent. Musk had built an honest-to-God rocket factory in the middle of Los Angeles.
And this factory was not making one rocket at a time. No. It was making many rockets—from scratch.
The factory was a giant, shared work area. Near the back were massive delivery bays that allowed
for the arrival of hunks of metal, which were transported to two-story-high welding machines. Over
to one side were technicians in white coats making motherboards, radios, and other electronics. Other
people were in a special, airtight glass chamber, building the capsules that rockets would take to the
Space Station. Tattooed men in bandanas were blasting Van Halen and threading wires around rocket
engines. There were completed bodies of rockets lined up one after the other ready to be placed on
trucks. Still more rockets, in another part of the building, awaited coats of white paint. It was difficult
to take in the entire factory at once. There were hundreds of bodies in constant motion whirring
around a variety of bizarre machines.
This is just building number one of Musk Land. SpaceX had acquired several buildings that used
to be part of a Boeing factory, which made the fuselages for 747s. One of these buildings has a curved
roof and looks like an airplane hangar. It serves as the research, development, and design studio for
Tesla. This is where the company came up with the look for the Model S sedan and its follow-on, the
Model X SUV. In the parking lot outside the studio, Tesla has built one of its recharging stations
where Los Angeles drivers can top up with electricity for free. The charging center is easy enough to
spot because Musk has installed a white and red obelisk branded with the Tesla logo that sits in the
middle of an infinity pool.
It was in my first interview with Musk, which took place at the design studio, that I began to get a
sense of how he talked and operated. He’s a confident guy, but does not always do a good job of
displaying this. On initial encounter, Musk can come off as shy and borderline awkward. His South
African accent remains present but fading, and the charm of it is not enough to offset the halting nature
of Musk’s speech pattern. Like many an engineer or physicist, Musk will pause while fishing around
for exact phrasing, and he’ll often go rumbling down an esoteric, scientific rabbit hole without
providing any helping hands or simplified explanations along the way. Musk expects you to keep up.
None of this is off-putting. Musk, in fact, will toss out plenty of jokes and can be downright charming.
It’s just that there’s a sense of purpose and pressure hanging over any conversation with the man.
Musk doesn’t really shoot the shit. (It would end up taking about thirty hours of interviews for Musk
to really loosen up and let me into a different, deeper level of his psyche and personality.)
Most high-profile CEOs have handlers all around them. Musk mostly moves about Musk Land on
his own. This is not the guy who slinks into the restaurant. It’s the guy who owns the joint and strides
about with authority. Musk and I talked, as he made his way around the design studio’s main floor,
inspecting prototype parts and vehicles. At each station, employees rushed up to Musk and disgorged
information. He listened intently, processed it, and nodded when satisfied. The people moved away
and Musk moved to the next information dump. At one point, Tesla’s design chief, Franz von
Holzhausen, wanted Musk’s take on some new tires and rims that had come in for the Model S and on
the seating arrangements for the Model X. They spoke, and then they went into a back room where
executives from a seller of high-end graphics software had prepared a presentation for Musk. They
wanted to show off new 3-D rendering technology that would allow Tesla to tweak the finish of a
virtual Model S and see in great detail how things like shadows and streetlights played off the car’s
body. Tesla’s engineers really wanted the computing systems and needed Musk’s sign-off. The men
did their best to sell Musk on the idea while the sound of drills and giant industrial fans drowned out
their shtick. Musk, wearing leather shoes, designer jeans, and a black T-shirt, which is essentially his
work uniform, had to don 3-D goggles for the demonstration and seemed unmoved. He told them he’d
think about it and then walked toward the source of the loudest noise—a workshop deep in the design
studio where Tesla engineers were building the scaffolding for the thirty-foot decorative towers that
go outside the charging stations. “That thing looks like it could survive a Category Five hurricane,”
Musk said. “Let’s thin it up a bit.” Musk and I eventually hop into his car—a black Model S—and zip
back to the main SpaceX building. “I think there are probably too many smart people pursuing Internet
stuff, finance, and law,” Musk said on the way. “That is part of the reason why we haven’t seen as
much innovation.”
MUSK LAND WAS A REVELATION.
I’d come to Silicon Valley in 2000 and ended up living in the Tenderloin neighborhood of San
Francisco. It’s the one part of the city that locals will implore you to avoid. Without trying very hard,
you can find someone pulling down his pants and pooping in between parked cars or encounter some
deranged sort bashing his head into the side of a bus stop. At dive bars near the local strip clubs,
transvestites hit on curious businessmen and drunks fall asleep on couches and soil themselves as part
of their lazy Sunday ritual. It’s the gritty, knife-stabby part of San Francisco and turned out to be a
great place to watch the dotcom dream die.
San Francisco has an enduring history with greed. It became a city on the back of the gold rush,
and not even a catastrophic earthquake could slow San Francisco’s economic lust for long. Don’t let
the granola vibes fool you. Booms and busts are the rhythm of this place. And, in 2000, San Francisco
had been overtaken by the boom of all booms and consumed by avarice. It was a wonderful time to be
alive with just about the entire populace giving in to a fantasy—a get-rich-quick, Internet madness.
The pulses of energy from this shared delusion were palpable, producing a constant buzz that vibrated
across the city. And here I was in the center of the most depraved part of San Francisco, watching just
how high and low people get when consumed by excess.
Stories tracking the insanity of business in these times are well-known. You no longer had to make
something that other people wanted to buy in order to start a booming company. You just had to have
an idea for some sort of Internet thing and announce it to the world in order for eager investors to fund
your thought experiment. The whole goal was to make as much money as possible in the shortest
amount of time because everyone knew on at least a subconscious level that reality had to set in
eventually.
Valley denizens took very literally the cliché of working as hard as you play. People in their
twenties, thirties, forties, and fifties were expected to pull all-nighters. Cubicles were turned into
temporary homes, and personal hygiene was abandoned. Oddly enough, making Nothing appear to be
Something took a lot of work. But when the time to decompress arrived, there were plenty of options
for total debauchery. The hot companies and media powers of the time seemed locked in a struggle to
outdo each other with ever-fancier parties. Old-line companies trying to look “with it” would
regularly buy space at a concert venue and then order up some dancers, acrobats, open bars, and the
Barenaked Ladies. Young technologists would show up to pound their free Jack and Cokes and snort
their cocaine in porta-potties. Greed and self-interest were the only things that made any sense back
then.
While the good times have been well chronicled, the subsequent bad times have been—
unsurprisingly—ignored. It’s more fun to reminiscence on irrational exuberance than the mess that
gets left behind.
Let it be said for the record, then, that the implosion of the get-rich-quick Internet fantasy left San
Francisco and Silicon Valley in a deep depression. The endless parties ended. The prostitutes no
longer roamed the streets of the Tenderloin at 6 A.M. offering pre-commute love. (“Come on, honey.
It’s better than coffee!”) Instead of the Barenaked Ladies, you got the occasional Neil Diamond tribute
band at a trade show, some free T-shirts, and a lump of shame.
The technology industry had no idea what to do with itself. The dumb venture capitalists who had
been taken during the bubble didn’t want to look any dumber, so they stopped funding new ventures
altogether. Entrepreneurs’ big ideas were replaced by the smallest of notions. It was as if Silicon
Valley had entered rehab en masse. It sounds melodramatic, but it’s true. A populace of millions of
clever people came to believe that they were inventing the future. Then . . . poof! Playing it safe
suddenly became the fashionable thing to do.
The evidence of this malaise is in the companies and ideas formed during this period. Google had
appeared and really started to thrive around 2002, but it was an outlier. Between Google and Apple’s
introduction of the iPhone in 2007, there’s a wasteland of ho-hum companies. And the hot new things
that were just starting out—Facebook and Twitter—certainly did not look like their predecessors—
Hewlett-Packard, Intel, Sun Microsystems—that made physical products and employed tens of
thousands of people in the process. In the years that followed, the goal went from taking huge risks to
create new industries and grand new ideas, to chasing easier money by entertaining consumers and
pumping out simple apps and advertisements. “The best minds of my generation are thinking about
how to make people click ads,” Jeff Hammerbacher, an early Facebook engineer, told me. “That
sucks.” Silicon Valley began to look an awful lot like Hollywood. Meanwhile, the consumers it
served had turned inward, obsessed with their virtual lives.
One of the first people to suggest that this lull in innovation could signal a much larger problem
was Jonathan Huebner, a physicist who works at the Pentagon’s Naval Air Warfare Center in China
Lake, California. Huebner is the Leave It to Beaver version of a merchant of death. Middle-aged,
thin, and balding, he likes to wear a dirt-inspired ensemble of khaki pants, a brown-striped shirt, and
a canvas khaki jacket. He has designed weapons systems since 1985, gaining direct insight into the
latest and greatest technology around materials, energy, and software. Following the dot-com bust, he
became miffed at the ho-hum nature of the supposed innovations crossing his desk. In 2005, Huebner
delivered a paper, “A Possible Declining Trend in Worldwide Innovation,” which was either an
indictment of Silicon Valley or at least an ominous warning.
Huebner opted to use a tree metaphor to describe what he saw as the state of innovation. Man has
already climbed past the trunk of the tree and gone out on its major limbs, mining most of the really
big, game-changing ideas—the wheel, electricity, the airplane, the telephone, the transistor. Now
we’re left dangling near the end of the branches at the top of the tree and mostly just refining past
inventions. To back up his point in the paper, Huebner showed that the frequency of life-changing
inventions had started to slow. He also used data to prove that the number of patents filed per person
had declined over time. “I think the probability of us discovering another top-one-hundred-type
invention gets smaller and smaller,” Huebner told me in an interview. “Innovation is a finite
resource.”
Huebner predicted that it would take people about five years to catch on to his thinking, and this
forecast proved almost exactly right. Around 2010, Peter Thiel, the PayPal cofounder and early
Facebook investor, began promoting the idea that the technology industry had let people down. “We
wanted flying cars, instead we got 140 characters” became the tagline of his venture capital firm
Founders Fund. In an essay called “What Happened to the Future,” Thiel and his cohorts described
how Twitter, its 140-character messages, and similar inventions have let the public down. He argued
that science fiction, which once celebrated the future, has turned dystopian because people no longer
have an optimistic view of technology’s ability to change the world.
I’d subscribed to a lot of this type of thinking until that first visit to Musk Land. While Musk had
been anything but shy about what he was up to, few people outside of his companies got to see the
factories, the R&D centers, the machine shops, and to witness the scope of what he was doing
firsthand. Here was a guy who had taken much of the Silicon Valley ethic behind moving quickly and
running organizations free of bureaucratic hierarchies and applied it to improving big, fantastic
machines and chasing things that had the potential to be the real breakthroughs we’d been missing.
By rights, Musk should have been part of the malaise. He jumped right into dot-com mania in
1995, when, fresh out of college, he founded a company called Zip2—a primitive Google Maps
meets Yelp. That first venture ended up a big, quick hit. Compaq bought Zip2 in 1999 for $307
million. Musk made $22 million from the deal and poured almost all of it into his next venture, a
start-up that would morph into PayPal. As the largest shareholder in PayPal, Musk became
fantastically well-to-do when eBay acquired the company for $1.5 billion in 2002.
Instead of hanging around Silicon Valley and falling into the same funk as his peers, however,
Musk decamped to Los Angeles. The conventional wisdom of the time said to take a deep breath and
wait for the next big thing to arrive in due course. Musk rejected that logic by throwing $100 million
into SpaceX, $70 million into Tesla, and $10 million into SolarCity. Short of building an actual
money-crushing machine, Musk could not have picked a faster way to destroy his fortune. He became
a one-man, ultra-risk-taking venture capital shop and doubled down on making super-complex
physical goods in two of the most expensive places in the world, Los Angeles and Silicon Valley.
Whenever possible, Musk’s companies would make things from scratch and try to rethink much that
the aerospace, automotive, and solar industries had accepted as convention.
With SpaceX, Musk is battling the giants of the U.S. military-industrial complex, including
Lockheed Martin and Boeing. He’s also battling nations—most notably Russia and China. SpaceX has
made a name for itself as the low-cost supplier in the industry. But that, in and of itself, is not really
good enough to win. The space business requires dealing with a mess of politics, back-scratching,
and protectionism that undermines the fundamentals of capitalism. Steve Jobs faced similar forces
when he went up against the recording industry to bring the iPod and iTunes to market. The crotchety
Luddites in the music industry were a pleasure to deal with compared to Musk’s foes who build
weapons and countries for a living. SpaceX has been testing reusable rockets that can carry payloads
to space and land back on Earth, on their launchpads, with precision. If the company can perfect this
technology, it will deal a devastating blow to all of its competitors and almost assuredly push some
mainstays of the rocket industry out of business while establishing the United States as the world
leader for taking cargo and humans to space. It’s a threat that Musk figures has earned him plenty of
fierce enemies. “The list of people that would not mind if I was gone is growing,” Musk said. “My
family fears that the Russians will assassinate me.”
With Tesla Motors, Musk has tried to revamp the way cars are manufactured and sold, while
building out a worldwide fuel distribution network at the same time. Instead of hybrids, which in
Musk lingo are suboptimal compromises, Tesla strives to make all-electric cars that people lust after
and that push the limits of technology. Tesla does not sell these cars through dealers; it sells them on
the Web and in Apple-like galleries located in high-end shopping centers. Tesla also does not
anticipate making lots of money from servicing its vehicles, since electric cars do not require the oil
changes and other maintenance procedures of traditional cars. The direct sales model embraced by
Tesla stands as a major affront to car dealers used to haggling with their customers and making their
profits from exorbitant maintenance fees. Tesla’s recharging stations now run alongside many of the
major highways in the United States, Europe, and Asia and can add hundreds of miles of oomph back
to a car in about twenty minutes. These so-called supercharging stations are solar-powered, and Tesla
owners pay nothing to refuel. While much of America’s infrastructure decays, Musk is building a
futuristic end-to-end transportation system that would allow the United States to leapfrog the rest of
the world. Musk’s vision, and, of late, execution seem to combine the best of Henry Ford and John D.
Rockefeller.
With SolarCity, Musk has funded the largest installer and financer of solar panels for consumers
and businesses. Musk helped come up with the idea for SolarCity and serves as its chairman, while
his cousins Lyndon and Peter Rive run the company. SolarCity has managed to undercut dozens of
utilities and become a large utility in its own right. During a time in which clean-tech businesses have
gone bankrupt with alarming regularity, Musk has built two of the most successful clean-tech
companies in the world. The Musk Co. empire of factories, tens of thousands of workers, and
industrial might has incumbents on the run and has turned Musk into one of the richest men in the
world, with a net worth around $10 billion.
The visit to Musk Land started to make a few things clear about how Musk had pulled all this off.
While the “putting man on Mars” talk can strike some people as loopy, it gave Musk a unique rallying
cry for his companies. It’s the sweeping goal that forms a unifying principle over everything he does.
Employees at all three companies are well aware of this and well aware that they’re trying to achieve
the impossible day in and day out. When Musk sets unrealistic goals, verbally abuses employees, and
works them to the bone, it’s understood to be—on some level—part of the Mars agenda. Some
employees love him for this. Others loathe him but remain oddly loyal out of respect for his drive and
mission. What Musk has developed that so many of the entrepreneurs in Silicon Valley lack is a
meaningful worldview. He’s the possessed genius on the grandest quest anyone has ever concocted.
He’s less a CEO chasing riches than a general marshaling troops to secure victory. Where Mark
Zuckerberg wants to help you share baby photos, Musk wants to . . . well . . . save the human race
from self-imposed or accidental annihilation.
The life that Musk has created to manage all of these endeavors is preposterous. A typical week
starts at his mansion in Bel Air. On Monday, he works the entire day at SpaceX. On Tuesday, he
begins at SpaceX, then hops onto his jet and flies to Silicon Valley. He spends a couple of days
working at Tesla, which has its offices in Palo Alto and factory in Fremont. Musk does not own a
home in Northern California and ends up staying at the luxe Rosewood hotel or at friends’ houses. To
arrange the stays with friends, Musk’s assistant will send an e-mail asking, “Room for one?” and if
the friend says, “Yes,” Musk turns up at the door late at night. Most often he stays in a guest room, but
he’s also been known to crash on the couch after winding down with some video games. Then it’s
back to Los Angeles and SpaceX on Thursday. He shares custody of his five young boys—twins and
triplets—with his ex-wife, Justine, and has them four days a week. Each year, Musk tabulates the
amount of flight time he endures per week to help him get a sense of just how out of hand things are
getting. Asked how he survives this schedule, Musk said, “I had a tough childhood, so maybe that was
helpful.”
During one visit to Musk Land, he had to squeeze our interview in before heading off for a
camping trip at Crater Lake National Park in Oregon. It was almost 8 P.M. on a Friday, so Musk would
soon be piling his boys and nannies into his private jet and then meeting drivers who would take him
to his friends at the campsite; the friends would then help the Musk clan unpack and complete their
pitch-black arrival. There would be a bit of hiking over the weekend. Then the relaxation would end.
Musk would fly with the boys back to Los Angeles on Sunday afternoon. Then, he would take off on
his own that evening for New York. Sleep. Hit the morning talk shows on Monday. Meetings. E-mail.
Sleep. Fly back to Los Angeles Tuesday morning. Work at SpaceX. Fly to San Jose Tuesday afternoon
to visit the Tesla Motors factory. Fly to Washington, D.C., that night and see President Obama. Fly
back to Los Angeles Wednesday night. Spend a couple of days working at SpaceX. Then go to a
weekend conference held by Google’s chairman, Eric Schmidt, in Yellowstone. At this time, Musk
had just split from his second wife, the actress Talulah Riley, and was trying to calculate if he could
mix a personal life into all of this. “I think the time allocated to the businesses and the kids is going
fine,” Musk said. “I would like to allocate more time to dating, though. I need to find a girlfriend.
That’s why I need to carve out just a little more time. I think maybe even another five to ten—how
much time does a woman want a week? Maybe ten hours? That’s kind of the minimum? I don’t know.”
Musk rarely finds time to decompress, but when he does, the festivities are just as dramatic as the
rest of his life. On his thirtieth birthday, Musk rented out a castle in England for about twenty people.
From 2 A.M. until 6 A.M., they played a variation of hide-and-seek called sardines in which one
person runs off and hides and everyone else looks for him. Another party occurred in Paris. Musk, his
brother, and cousins found themselves awake at midnight and decided to bicycle through the city until
6 A.M. They slept all day and then boarded the Orient Express in the evening. Once again, they stayed
up all night. The Lucent Dossier Experience—an avant-garde group of performers—were on the
luxurious train, performing palm readings and acrobatics. When the train arrived in Venice the next
day, Musk’s family had dinner and then hung out on the patio of their hotel overlooking the Grand
Canal until 9 A.M. Musk loves costume parties as well, and turned up at one dressed like a knight and
using a parasol to duel a midget wearing a Darth Vader costume.
For one of his most recent birthdays, Musk invited fifty people to a castle—or at least the United
States’ best approximation of a castle—in Tarrytown, New York. This party had a Japanese
steampunk theme, which is sort of like a sci-fi lover’s wet dream—a mix of corsets, leather, and
machine worship. Musk dressed as a samurai.
The festivities included a performance of The Mikado, a Victorian comic opera by Gilbert and
Sullivan set in Japan, at a small theater in the heart of town. “I am not sure the Americans got it,” said
Riley, whom Musk remarried after his ten-hour-a-week dating plan failed. The Americans and
everyone else did enjoy what followed. Back at the castle, Musk donned a blindfold, got pushed up
against a wall, and held balloons in each hand and another between his legs. The knife thrower then
went to work. “I’d seen him before, but did worry that maybe he could have an off day,” Musk said.
“Still, I thought, he would maybe hit one gonad but not both.” The onlookers were stunned and
frightened for Musk’s safety. “That was bizarre,” said Bill Lee, a technology investor and one of
Musk’s good friends. “But Elon believes in the science of things.” One of the world’s top sumo
wrestlers showed up at the party along with some of his compatriots. A ring had been set up at the
castle, and Musk faced off against the champion. “He was three hundred and fifty pounds, and they
were not jiggly pounds,” Musk said. “I went full adrenaline rush and managed to lift the guy off the
ground. He let me win that first round and then beat me. I think my back is still screwed up.”
Riley turned planning these types of parties for Musk into an art. She met Musk back in 2008,
when his companies were collapsing. She watched him lose his entire fortune and get ridiculed by the
press. She knows that the sting of these years remains and has combined with the other traumas in
Musk’s life—the tragic loss of an infant son and a brutal upbringing in South Africa—to create a
tortured soul. Riley has gone to great lengths to make sure Musk’s escapes from work and this past
leave him feeling refreshed if not healed. “I try to think of fun things he has not done before where he
can relax,” Riley said. “We’re trying to make up for his miserable childhood now.”
Genuine as Riley’s efforts might have been, they were not entirely effective. Not long after the
Sumo party, I found Musk back at work at the Tesla headquarters in Palo Alto. It was a Saturday, and
the parking lot was full of cars. Inside of the Tesla offices, hundreds of young men were at work—
some of them designing car parts on computers and others conducting experiments with electronics
equipment on their desks. Musk’s uproarious laugh would erupt every few minutes and carry through
the entire floor. When Musk came into the meeting room where I’d been waiting, I noted how
impressive it was for so many people to turn up on a Saturday. Musk saw the situation in a different
light, complaining that fewer and fewer people had been working weekends of late. “We’ve grown
fucking soft,” Musk replied. “I was just going to send out an e-mail. We’re fucking soft.” (A word of
warning: There’s going to be a lot of “fuck” in this book. Musk adores the word, and so do most of
the people in his inner circle.)
This kind of declaration seems to fit with our impressions of other visionaries. It’s not hard to
imagine Howard Hughes or Steve Jobs chastising their workforce in a similar way. Building things—
especially big things—is a messy business. In the two decades Musk has spent creating companies,
he’s left behind a trail of people who either adore or despise him. During the course of my reporting,
these people lined up to give me their take on Musk and the gory details of how he and his businesses
operate.
My dinners with Musk and periodic trips to Musk Land revealed a different set of possible truths
about the man. He’s set about building something that has the potential to be much grander than
anything Hughes or Jobs produced. Musk has taken industries like aerospace and automotive that
America seemed to have given up on and recast them as something new and fantastic. At the heart of
this transformation are Musk’s skills as a software maker and his ability to apply them to machines.
He’s merged atoms and bits in ways that few people thought possible, and the results have been
spectacular. It’s true enough that Musk has yet to have a consumer hit on the order of the iPhone or to
touch more than one billion people like Facebook. For the moment, he’s still making rich people’s
toys, and his budding empire could be an exploded rocket or massive Tesla recall away from
collapse. On the other hand, Musk’s companies have already accomplished far more than his loudest
detractors thought possible, and the promise of what’s to come has to leave hardened types feeling
optimistic during their weaker moments. “To me, Elon is the shining example of how Silicon Valley
might be able to reinvent itself and be more relevant than chasing these quick IPOs and focusing on
getting incremental products out,” said Edward Jung, a famed software engineer and inventor. “Those
things are important, but they are not enough. We need to look at different models of how to do things
that are longer term in nature and where the technology is more integrated.” The integration mentioned
by Jung—the harmonious melding of software, electronics, advanced materials, and computing
horsepower—appears to be Musk’s gift. Squint ever so slightly, and it looks like Musk could be using
his skills to pave the way toward an age of astonishing machines and science fiction dreams made
manifest.
In that sense, Musk comes off much more like Thomas Edison than Howard Hughes. He’s an
inventor, celebrity businessman, and industrialist able to take big ideas and turn them into big
products. He’s employing thousands of people to forge metal in American factories at a time when
this was thought to be impossible. Born in South Africa, Musk now looks like America’s most
innovative industrialist and outlandish thinker and the person most likely to set Silicon Valley on a
more ambitious course. Because of Musk, Americans could wake up in ten years with the most
modern highway in the world: a transit system run by thousands of solar-powered charging stations
and traversed by electric cars. By that time, SpaceX may well be sending up rockets every day, taking
people and things to dozens of habitats and making preparations for longer treks to Mars. These
advances are simultaneously difficult to fathom and seemingly inevitable if Musk can simply buy
enough time to make them work. As his ex-wife, Justine, put it, “He does what he wants, and he is
relentless about it. It’s Elon’s world, and the rest of us live in it.”
2
AFRICA
THE PUBLIC FIRST MET ELON REEVE MUSK IN 1984. The South African trade publication PC
and Of ice Technology published the source code to a video game Musk had designed. Called
Blastar, the science-fiction-inspired space game required 167 lines of instructions to run. This was
back in the day when early computer users were required to type out commands to make their
machines do much of anything. In that context, Musk’s game did not shine as a marvel of computer
science but it certainly surpassed what most twelve-year-olds were kicking out at the time. Its
coverage in the magazine netted Musk five hundred dollars and provided some early hints about his
character. The Blastar spread on page 69 of the magazine shows that the young man wanted to go by
the sci-fi-author-sounding name E. R. Musk and that he already had visions of grand conquests
dancing in his head. The brief explainer states, “In this game you have to destroy an alien space
freighter, which is carrying deadly Hydrogen Bombs and Status Beam Machines. This game makes
good use of sprites and animation, and in this sense makes the listing worth reading.” (As of this
writing, not even the Internet knows what “status beam machines” are.)

 
 </h2>
<p>
<pre>


import numpy as np
import matplotlib.pyplot as plt
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from keras.utils.np_utils import to_categorical
import random


np.random.seed(0)




(X_train, y_train), (X_test, y_test)= mnist.load_data()
 
print(X_train.shape)
print(X_test.shape)
assert(X_train.shape[0] == y_train.shape[0]), "The number of images is not equal to the number of labels."
assert(X_train.shape[1:] == (28,28)), "The dimensions of the images are not 28 x 28."
assert(X_test.shape[0] == y_test.shape[0]), "The number of images is not equal to the number of labels."
assert(X_test.shape[1:] == (28,28)), "The dimensions of the images are not 28 x 28."
 
num_of_samples=[]
 
cols = 5
num_classes = 10
 
fig, axs = plt.subplots(nrows=num_classes, ncols=cols, figsize=(5,10))
fig.tight_layout()
 
for i in range(cols):
    for j in range(num_classes):
      x_selected = X_train[y_train == j]
      axs[j][i].imshow(x_selected[random.randint(0,(len(x_selected) - 1)), :, :], cmap=plt.get_cmap('gray'))
      axs[j][i].axis("off")
      if i == 2:
        axs[j][i].set_title(str(j))
        num_of_samples.append(len(x_selected))

print(num_of_samples)
plt.figure(figsize=(12, 4))
plt.bar(range(0, num_classes), num_of_samples)
plt.title("Distribution of the train dataset")
plt.xlabel("Class number")
plt.ylabel("Number of images")
plt.show()
 


X_train = X_train.reshape(60000, 28, 28, 1)
X_test = X_test.reshape(10000, 28, 28, 1)


y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)
 
X_train = X_train/255
X_test = X_test/255


End of starter code

__________________________________________________________________________________________________________


Below is the previous MNIST Code which you might end up reusing in the next video.  (It's purpose will be more clear as you enter the next video):



import numpy as np
import matplotlib.pyplot as plt
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from keras.utils.np_utils import to_categorical
import random
np.random.seed(0)
(X_train, y_train), (X_test, y_test) = mnist.load_data()
 
print(X_train.shape)
print(X_test.shape)
print(y_train.shape[0])
assert(X_train.shape[0] == y_train.shape[0]), "The number of images is not equal to the number of labels."
assert(X_test.shape[0] == y_test.shape[0]), "The number of images is not equal to the number of labels."
assert(X_train.shape[1:] == (28,28)), "The dimensions of the images are not 28x28"
assert(X_test.shape[1:] == (28,28)), "The dimensions of the images are not 28x28"
num_of_samples = []
 
cols = 5
num_classes = 10
 
fig, axs = plt.subplots(nrows=num_classes, ncols = cols, figsize=(5, 8))
fig.tight_layout()
for i in range(cols):
    for j in range(num_classes):
        x_selected = X_train[y_train == j]
        axs[j][i].imshow(x_selected[random.randint(0, len(x_selected - 1)), :, :], cmap=plt.get_cmap("gray"))
        axs[j][i].axis("off")
        if i == 2:
            axs[j][i].set_title(str(j))
            num_of_samples.append(len(x_selected))
print(num_of_samples)
plt.figure(figsize=(12, 4))
plt.bar(range(0, num_classes), num_of_samples)
plt.title("Distribution of the training dataset")
plt.xlabel("Class number")
plt.ylabel("Number of images")
 
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)
 
X_train = X_train/255 
X_test = X_test/255
 
num_pixels = 784
X_train = X_train.reshape(X_train.shape[0], num_pixels)
X_test = X_test.reshape(X_test.shape[0], num_pixels)
def create_model():
    model = Sequential()
    model.add(Dense(10, input_dim=num_pixels, activation='relu'))
    model.add(Dense(30, activation='relu'))
    model.add(Dense(10, activation='relu'))
    model.add(Dense(num_classes, activation='softmax'))
    model.compile(Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
 
model = create_model()
print(model.summary())
 
history = model.fit(X_train, y_train, validation_split=0.1, epochs = 10, batch_size = 200, verbose = 1, shuffle = 1)
 
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['loss', 'val_loss'])
plt.title('Loss')
plt.xlabel('epoch')
 
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.legend(['acc', 'val_acc'])
plt.title('Accuracy')
plt.xlabel('epoch')
 
score = model.evaluate(X_test, y_test, verbose=0)
print(type(score))
print('Test score:', score[0])
print('Test accuracy:', score[1])
 
import requests
from PIL import Image
 
url = 'https://www.researchgate.net/profile/Jose_Sempere/publication/221258631/figure/fig1/AS:305526891139075@1449854695342/Handwritten-digit-2.png'
response = requests.get(url, stream=True)
img = Image.open(response.raw)
plt.imshow(img, cmap=plt.get_cmap('gray'))
 
import cv2
 
img = np.asarray(img)
img = cv2.resize(img, (28, 28))
img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
img = cv2.bitwise_not(img)
plt.imshow(img, cmap=plt.get_cmap('gray'))
 
img = img/255
img = img.reshape(1, 784)
 
prediction = model.predict_classes(img)
print("predicted digit:", str(prediction))
  </code>
  </p>

<h2>Code2</h2>
  <p>

    
import numpy as np
import matplotlib.pyplot as plt
import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from keras.utils.np_utils import to_categorical
import random
 
from keras.layers import Dropout
from keras.layers import Flatten
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
 
from keras.models import Model
np.random.seed(0)
(X_train, y_train), (X_test, y_test)= mnist.load_data()
 
print(X_train.shape)
print(X_test.shape)
# STOP: Do not change the tests below. Your implementation should pass these tests. 
assert(X_train.shape[0] == y_train.shape[0]), "The number of images is not equal to the number of labels."
assert(X_train.shape[1:] == (28,28)), "The dimensions of the images are not 28 x 28."
assert(X_test.shape[0] == y_test.shape[0]), "The number of images is not equal to the number of labels."
assert(X_test.shape[1:] == (28,28)), "The dimensions of the images are not 28 x 28."
num_of_samples=[]
 
cols = 5
num_classes = 10
 
fig, axs = plt.subplots(nrows=num_classes, ncols=cols, figsize=(5,10))
fig.tight_layout()
 
for i in range(cols):
    for j in range(num_classes):
      x_selected = X_train[y_train == j]
      axs[j][i].imshow(x_selected[random.randint(0,(len(x_selected) - 1)), :, :], cmap=plt.get_cmap('gray'))
      axs[j][i].axis("off")
      if i == 2:
        axs[j][i].set_title(str(j))
        num_of_samples.append(len(x_selected))
print(num_of_samples)
plt.figure(figsize=(12, 4))
plt.bar(range(0, num_classes), num_of_samples)
plt.title("Distribution of the train dataset")
plt.xlabel("Class number")
plt.ylabel("Number of images")
plt.show()
X_train = X_train.reshape(60000, 28, 28, 1)
X_test = X_test.reshape(10000, 28, 28, 1)
 
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)
 
X_train = X_train/255
X_test = X_test/255
# define the larger model
def leNet_model():
  # create model
  model = Sequential()
  model.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  
  model.add(Conv2D(15, (3, 3), activation='relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  
  model.add(Flatten())
  model.add(Dense(500, activation='relu'))
  model.add(Dropout(0.5))
  model.add(Dense(num_classes, activation='softmax'))
  # Compile model
  model.compile(Adam(lr = 0.01), loss='categorical_crossentropy', metrics=['accuracy'])
  return model
model = leNet_model()
print(model.summary())
history=model.fit(X_train, y_train, epochs=10,  validation_split = 0.1, batch_size = 400, verbose = 1, shuffle = 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['training', 'validation'])
plt.title('Loss')
plt.xlabel('epoch')
 
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.legend(['training','validation'])
plt.title('Accuracy')
plt.xlabel('epoch')
score = model.evaluate(X_test, y_test, verbose=0)
 
print('Test score:', score[0])
print('Test accuracy:', score[1])
#predict internet number
import requests
from PIL import Image
url = 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcST8KzXHtkSHcxzdpnllMhAj0upLEwnNFdtY6j4YUPcmaf4Ty3u'
 
r = requests.get(url, stream=True)
img = Image.open(r.raw)
plt.imshow(img, cmap=plt.get_cmap('gray'))
 
import cv2
 
img = np.asarray(img)
img = cv2.resize(img, (28, 28))
img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
img = cv2.bitwise_not(img)
plt.imshow(img, cmap=plt.get_cmap('gray'))
 
img = img/255
img = img.reshape(1,28,28,1)
print(img.shape)
 
print(img.shape)
print("predicted digit: "+str(model.predict_classes(img)))
 
layer1 = Model(inputs=model.layers[0].input, outputs=model.layers[0].output)
layer2 = Model(inputs=model.layers[0].input, outputs=model.layers[2].output)
 
visual_layer1, visual_layer2 = layer1.predict(img), layer2.predict(img)
 
print(visual_layer1.shape)
print(visual_layer2.shape)
 
#layer 1
plt.figure(figsize=(10, 6))
for i in range(30):
    plt.subplot(6, 5, i+1)
    plt.imshow(visual_layer1[0, :, :, i], cmap=plt.get_cmap('jet'))
    plt.axis('off')
 
#layer 2
plt.figure(figsize=(10, 6))
for i in range(15):
    plt.subplot(3, 5, i+1)
    plt.imshow(visual_layer2[0, :, :, i], cmap=plt.get_cmap('jet'))
    plt.axis('off')

    
    
    
      </pre>
</p>


<p>
--------------------------------------------------------------------------
</p>

<h2>Code12-1</h2>
<p>
<pre>
You will require this snippet of code for the next lesson:

import numpy as np
import matplotlib.pyplot as plt
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from keras.utils.np_utils import to_categorical
from keras.layers import Dropout, Flatten
from keras.layers.convolutional import Conv2D, MaxPooling2D
np.random.seed(0)
End of starter project

__________________________________________________________________________________________________________________



Some snippets of code you will require later on throughout the video:

num_of_samples = []
 
cols = 5
num_classes = 10
 
fig, axs = plt.subplots(nrows=num_classes, ncols = cols, figsize=(5, 8))
fig.tight_layout()
for i in range(cols):
    for j in range(num_classes):
        x_selected = X_train[y_train == j]
        axs[j][i].imshow(x_selected[random.randint(0, len(x_selected - 1)), :, :], cmap=plt.get_cmap("gray"))
        axs[j][i].axis("off")
        if i == 2:
            axs[j][i].set_title(str(j))
            num_of_samples.append(len(x_selected))


print(num_of_samples)
plt.figure(figsize=(12, 4))
plt.bar(range(0, num_classes), num_of_samples)
plt.title("Distribution of the training dataset")
plt.xlabel("Class number")
plt.ylabel("Number of images")
</pre>
</p>

<p>
--------------------------------------------------------------------------
</p>

<h2>Code12-2</h2>
<p>
<pre>
2. You will also require the following sample code to fetch images from the web:

#fetch image
 
import requests
from PIL import Image
url = 'https://c8.alamy.com/comp/J2MRAJ/german-road-sign-bicycles-crossing-J2MRAJ.jpg'
r = requests.get(url, stream=True)
img = Image.open(r.raw)
plt.imshow(img, cmap=plt.get_cmap('gray'))
 
 
#Preprocess image
 
img = np.asarray(img)
img = cv2.resize(img, (32, 32))
img = preprocessing(img)
plt.imshow(img, cmap = plt.get_cmap('gray'))
print(img.shape)
 
#Reshape reshape
 
img = img.reshape(1, 32, 32, 1)
 
#Test image
print("predicted sign: "+ str(model.predict_classes(img)))
</pre>
</p>

<p>
--------------------------------------------------------------------------
</p>

<h2>Code12-3</h2>
<p>
<pre>
Final code for Traffic Signs Classification:



!git clone https://bitbucket.org/jadslim/german-traffic-signs
!ls german-traffic-sign
import numpy as np
import matplotlib.pyplot as plt
import keras
from keras.models import Sequential
from keras.optimizers import Adam
from keras.layers import Dense
from keras.layers import Flatten, Dropout
from keras.utils.np_utils import to_categorical
from keras.layers.convolutional import Conv2D, MaxPooling2D
import random
import pickle
import pandas as pd
import cv2
 
 
from keras.callbacks import LearningRateScheduler, ModelCheckpoint
 
%matplotlib inline
np.random.seed(0)
# TODO: Implement load the data here.
with open('german-traffic-signs/train.p', 'rb') as f:
    train_data = pickle.load(f)
with open('german-traffic-signs/valid.p', 'rb') as f:
    val_data = pickle.load(f)
# TODO: Load test data
wit
h open('german-traffic-signs/test.p', 'rb') as f:
    test_data = pickle.load(f)
 
    
# Split out features and labels
X_train, y_train = train_data['features'], train_data['labels']
X_val, y_val = val_data['features'], val_data['labels']
X_test, y_test = test_data['features'], test_data['labels']
 
#already 4 dimensional
print(X_train.shape)
print(X_test.shape)
print(X_val.shape)
# STOP: Do not change the tests below. Your implementation should pass these tests. 
assert(X_train.shape[0] == y_train.shape[0]), "The number of images is not equal to the number of labels."
assert(X_train.shape[1:] == (32,32,3)), "The dimensions of the images are not 32 x 32 x 3."
assert(X_val.shape[0] == y_val.shape[0]), "The number of images is not equal to the number of labels."
assert(X_val.shape[1:] == (32,32,3)), "The dimensions of the images are not 32 x 32 x 3."
assert(X_test.shape[0] == y_test.shape[0]), "The number of images is not equal to the number of labels."
assert(X_test.shape[1:] == (32,32,3)), "The dimensions of the images are not 32 x 32 x 3."
data = pd.read_csv('german-traffic-signs/signnames.csv')
  
  num_of_samples=[]
 
  cols = 5
  num_classes = 43
 
  fig, axs = plt.subplots(nrows=num_classes, ncols=cols, figsize=(5,50))
  fig.tight_layout()
 
  for i in range(cols):
      for j, row in data.iterrows():
        x_selected = X_train[y_train == j]
        axs[j][i].imshow(x_selected[random.randint(0,(len(x_selected) - 1)), :, :], cmap=plt.get_cmap('gray'))
        axs[j][i].axis("off")
        if i == 2:
          axs[j][i].set_title(str(j) + " - " + row["SignName"])
          num_of_samples.append(len(x_selected))
print(num_of_samples)
plt.figure(figsize=(12, 4))
plt.bar(range(0, num_classes), num_of_samples)
plt.title("Distribution of the train dataset")
plt.xlabel("Class number")
plt.ylabel("Number of images")
plt.show()
import cv2
 
plt.imshow(X_train[1000])
plt.axis("off")
print(X_train[1000].shape)
print(y_train[1000])
def grayscale(img):
    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    return img
img = grayscale(X_train[1000])
plt.imshow(img)
plt.axis("off")
print(img.shape)
def equalize(img):
    img = cv2.equalizeHist(img)
    return img
img = equalize(img)
plt.imshow(img)
plt.axis("off")
print(img.shape)
def preprocess(img):
    img = grayscale(img)
    img = equalize(img)
    img = img/255
    return img
  
X_train = np.array(list(map(preprocess, X_train)))
X_test = np.array(list(map(preprocess, X_test)))
X_val = np.array(list(map(preprocess, X_val)))
 
plt.imshow(X_train[random.randint(0, len(X_train) - 1)])
plt.axis('off')
print(X_train.shape)
X_train = X_train.reshape(34799, 32, 32, 1)
X_test = X_test.reshape(12630, 32, 32, 1)
X_val = X_val.reshape(4410, 32, 32, 1)
from keras.preprocessing.image import ImageDataGenerator
 
datagen = ImageDataGenerator(width_shift_range=0.1,
                            height_shift_range=0.1,
                            zoom_range=0.2,
                            shear_range=0.1,
                            rotation_range=10.)
 
datagen.fit(X_train)
# for X_batch, y_batch in
 
batches = datagen.flow(X_train, y_train, batch_size = 15)
X_batch, y_batch = next(batches)
 
fig, axs = plt.subplots(1, 15, figsize=(20, 5))
fig.tight_layout()
 
for i in range(15):
    axs[i].imshow(X_batch[i].reshape(32, 32))
    axs[i].axis("off")
 
print(X_batch.shape)
y_train = to_categorical(y_train, 43)
y_test = to_categorical(y_test, 43)
y_val = to_categorical(y_val, 43)
# create model
 
def modified_model():
  model = Sequential()
  model.add(Conv2D(60, (5, 5), input_shape=(32, 32, 1), activation='relu'))
  model.add(Conv2D(60, (5, 5), activation='relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  
  model.add(Conv2D(30, (3, 3), activation='relu'))
  model.add(Conv2D(30, (3, 3), activation='relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  
  model.add(Flatten())
  model.add(Dense(500, activation='relu'))
  model.add(Dropout(0.5))
  model.add(Dense(43, activation='softmax'))
  
  model.compile(Adam(lr = 0.001), loss='categorical_crossentropy', metrics=['accuracy'])
  return model
model = modified_model()
print(model.summary())
 
history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=50),
                            steps_per_epoch=2000,
                            epochs=10,
                            validation_data=(X_val, y_val), shuffle = 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss')
plt.xlabel('epoch')
 
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.legend(['training','test'])
plt.title('Accuracy')
plt.xlabel('epoch')
 
# TODO: Evaluate model on test data
score = model.evaluate(X_test, y_test, verbose=0)
print('Test score:', score[0])
print('Test accuracy:', score[1])
 
#predict internet number
import requests
from PIL import Image
url = 'https://c8.alamy.com/comp/A0RX23/cars-and-automobiles-must-turn-left-ahead-sign-A0RX23.jpg'
r = requests.get(url, stream=True)
img = Image.open(r.raw)
plt.imshow(img, cmap=plt.get_cmap('gray'))
 
img = np.asarray(img)
img = cv2.resize(img, (32, 32))
img = preprocess(img)
plt.imshow(img, cmap = plt.get_cmap('gray'))
print(img.shape)
img = img.reshape(1, 32, 32, 1)
 
print("predicted sign: "+ str(model.predict_classes(img)))
</pre>
</p>

<p>
--------------------------------------------------------------------------
</p>

<h2>Code13</h2>
<p>
<pre>
import numpy as np
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam


np.random.seed(0)
points = 500
X = np.linspace(-3, 3, points)
y = np.sin(X) + np.random.uniform(-0.5, 0.5, points)
model = Sequential()
model.add(Dense(50, activation='sigmoid', input_dim=1))
model.add(Dense(30, activation='sigmoid'))
model.add(Dense(1))


adam = Adam(lr=0.01)
model.compile(loss='mse', optimizer=adam)
model.fit(X, y, epochs=50)
predictions = model.predict(X)
plt.scatter(X, y)
plt.plot(X, predictions, 'ro')
plt.show()
</pre>
</p>

<p>
--------------------------------------------------------------------------
</p>

<h2>Code14-1</h2>
<p>
<pre>
In the next video, I introduce Flask & Socket.io to establish bi-directional client-server communication. Ultimately, this will be done to connect our model to the simulation.

That being said, the content in the next two videos is very technical, and not very relevant to deep learning. I would still highly recommend following along the two videos so that you don't get lost.

Otherwise, if you choose to skip the two videos, then that's fine. You'll have to simply copy the code below into an atom project, drag your model into the same project, make the appropriate installations and run the code to establish the connection. Then run your simulation.



import socketio
import eventlet
import numpy as np
from flask import Flask
from keras.models import load_model
import base64
from io import BytesIO
from PIL import Image
import cv2
 
sio = socketio.Server()
 
app = Flask(__name__) #'__main__'
speed_limit = 10
def img_preprocess(img):
    img = img[60:135,:,:]
    img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)
    img = cv2.GaussianBlur(img,  (3, 3), 0)
    img = cv2.resize(img, (200, 66))
    img = img/255
    return img
 
 
@sio.on('telemetry')
def telemetry(sid, data):
    speed = float(data['speed'])
    image = Image.open(BytesIO(base64.b64decode(data['image'])))
    image = np.asarray(image)
    image = img_preprocess(image)
    image = np.array([image])
    steering_angle = float(model.predict(image))
    throttle = 1.0 - speed/speed_limit
    print('{} {} {}'.format(steering_angle, throttle, speed))
    send_control(steering_angle, throttle)
 
 
 
@sio.on('connect')
def connect(sid, environ):
    print('Connected')
    send_control(0, 0)
 
def send_control(steering_angle, throttle):
    sio.emit('steer', data = {
        'steering_angle': steering_angle.__str__(),
        'throttle': throttle.__str__()
    })
 
 
if __name__ == '__main__':
    model = load_model('model.h5')
    app = socketio.Middleware(sio, app)
    eventlet.wsgi.server(eventlet.listen(('', 4567)), app)
</pre>
</p>

<p>
--------------------------------------------------------------------------
</p>

<h2>Code14-2</h2>
<p>
<pre>
Below is the final code for Behavioural Cloning

!git clone https://github.com/rslim087a/track
!ls track
!pip3 install imgaug
import os
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import keras
from keras.models import Sequential
from keras.optimizers import Adam
from keras.layers import Convolution2D, MaxPooling2D, Dropout, Flatten, Dense
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from imgaug import augmenters as iaa
import cv2
import pandas as pd
import ntpath
import random
datadir = 'track'
columns = ['center', 'left', 'right', 'steering', 'throttle', 'reverse', 'speed']
data = pd.read_csv(os.path.join(datadir, 'driving_log.csv'), names = columns)
pd.set_option('display.max_colwidth', -1)
data.head()
def path_leaf(path):
  head, tail = ntpath.split(path)
  return tail
data['center'] = data['center'].apply(path_leaf)
data['left'] = data['left'].apply(path_leaf)
data['right'] = data['right'].apply(path_leaf)
data.head()
num_bins = 25
samples_per_bin = 400
hist, bins = np.histogram(data['steering'], num_bins)
center = (bins[:-1]+ bins[1:]) * 0.5
plt.bar(center, hist, width=0.05)
plt.plot((np.min(data['steering']), np.max(data['steering'])), (samples_per_bin, samples_per_bin))
print('total data:', len(data))
remove_list = []
for j in range(num_bins):
  list_ = []
  for i in range(len(data['steering'])):
    if data['steering'][i] >= bins[j] and data['steering'][i] <= bins[j+1]:
      list_.append(i)
  list_ = shuffle(list_)
  list_ = list_[samples_per_bin:]
  remove_list.extend(list_)
 
print('removed:', len(remove_list))
data.drop(data.index[remove_list], inplace=True)
print('remaining:', len(data))
 
hist, _ = np.histogram(data['steering'], (num_bins))
plt.bar(center, hist, width=0.05)
plt.plot((np.min(data['steering']), np.max(data['steering'])), (samples_per_bin, samples_per_bin))


print(data.iloc[1])
def load_img_steering(datadir, df):
  image_path = []
  steering = []
  for i in range(len(data)):
    indexed_data = data.iloc[i]
    center, left, right = indexed_data[0], indexed_data[1], indexed_data[2]
    image_path.append(os.path.join(datadir, center.strip()))
    steering.append(float(indexed_data[3]))
    # left image append
    image_path.append(os.path.join(datadir,left.strip()))
    steering.append(float(indexed_data[3])+0.15)
    # right image append
    image_path.append(os.path.join(datadir,right.strip()))
    steering.append(float(indexed_data[3])-0.15)
  image_paths = np.asarray(image_path)
  steerings = np.asarray(steering)
  return image_paths, steerings
 
image_paths, steerings = load_img_steering(datadir + '/IMG', data)
X_train, X_valid, y_train, y_valid = train_test_split(image_paths, steerings, test_size=0.2, random_state=6)
print('Training Samples: {}\nValid Samples: {}'.format(len(X_train), len(X_valid)))
fig, axes = plt.subplots(1, 2, figsize=(12, 4))
axes[0].hist(y_train, bins=num_bins, width=0.05, color='blue')
axes[0].set_title('Training set')
axes[1].hist(y_valid, bins=num_bins, width=0.05, color='red')
axes[1].set_title('Validation set')


def zoom(image):
  zoom = iaa.Affine(scale=(1, 1.3))
  image = zoom.augment_image(image)
  return image
image = image_paths[random.randint(0, 1000)]
original_image = mpimg.imread(image)
zoomed_image = zoom(original_image)
 
fig, axs = plt.subplots(1, 2, figsize=(15, 10))
fig.tight_layout()
 
axs[0].imshow(original_image)
axs[0].set_title('Original Image')
 
axs[1].imshow(zoomed_image)
axs[1].set_title('Zoomed Image')


def pan(image):
  pan = iaa.Affine(translate_percent= {"x" : (-0.1, 0.1), "y": (-0.1, 0.1)})
  image = pan.augment_image(image)
  return image
image = image_paths[random.randint(0, 1000)]
original_image = mpimg.imread(image)
panned_image = pan(original_image)
 
fig, axs = plt.subplots(1, 2, figsize=(15, 10))
fig.tight_layout()
 
axs[0].imshow(original_image)
axs[0].set_title('Original Image')
 
axs[1].imshow(panned_image)
axs[1].set_title('Panned Image')
def img_random_brightness(image):
    brightness = iaa.Multiply((0.2, 1.2))
    image = brightness.augment_image(image)
    return image
image = image_paths[random.randint(0, 1000)]
original_image = mpimg.imread(image)
brightness_altered_image = img_random_brightness(original_image)
 
fig, axs = plt.subplots(1, 2, figsize=(15, 10))
fig.tight_layout()
 
axs[0].imshow(original_image)
axs[0].set_title('Original Image')
 
axs[1].imshow(brightness_altered_image)
axs[1].set_title('Brightness altered image ')


def img_random_flip(image, steering_angle):
    image = cv2.flip(image,1)
    steering_angle = -steering_angle
    return image, steering_angle
random_index = random.randint(0, 1000)
image = image_paths[random_index]
steering_angle = steerings[random_index]
 
 
original_image = mpimg.imread(image)
flipped_image, flipped_steering_angle = img_random_flip(original_image, steering_angle)
 
fig, axs = plt.subplots(1, 2, figsize=(15, 10))
fig.tight_layout()
 
axs[0].imshow(original_image)
axs[0].set_title('Original Image - ' + 'Steering Angle:' + str(steering_angle))
 
axs[1].imshow(flipped_image)
axs[1].set_title('Flipped Image - ' + 'Steering Angle:' + str(flipped_steering_angle))
def random_augment(image, steering_angle):
    image = mpimg.imread(image)
    if np.random.rand() < 0.5:
      image = pan(image)
    if np.random.rand() < 0.5:
      image = zoom(image)
    if np.random.rand() < 0.5:
      image = img_random_brightness(image)
    if np.random.rand() < 0.5:
      image, steering_angle = img_random_flip(image, steering_angle)
    
    return image, steering_angle
ncol = 2
nrow = 10
 
fig, axs = plt.subplots(nrow, ncol, figsize=(15, 50))
fig.tight_layout()
 
for i in range(10):
  randnum = random.randint(0, len(image_paths) - 1)
  random_image = image_paths[randnum]
  random_steering = steerings[randnum]
    
  original_image = mpimg.imread(random_image)
  augmented_image, steering = random_augment(random_image, random_steering)
    
  axs[i][0].imshow(original_image)
  axs[i][0].set_title("Original Image")
  
  axs[i][1].imshow(augmented_image)
  axs[i][1].set_title("Augmented Image")
 
def img_preprocess(img):
    img = img[60:135,:,:]
    img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)
    img = cv2.GaussianBlur(img,  (3, 3), 0)
    img = cv2.resize(img, (200, 66))
    img = img/255
    return img
image = image_paths[100]
original_image = mpimg.imread(image)
preprocessed_image = img_preprocess(original_image)
 
fig, axs = plt.subplots(1, 2, figsize=(15, 10))
fig.tight_layout()
axs[0].imshow(original_image)
axs[0].set_title('Original Image')
axs[1].imshow(preprocessed_image)
axs[1].set_title('Preprocessed Image')
def batch_generator(image_paths, steering_ang, batch_size, istraining):
  
  while True:
    batch_img = []
    batch_steering = []
    
    for i in range(batch_size):
      random_index = random.randint(0, len(image_paths) - 1)
      
      if istraining:
        im, steering = random_augment(image_paths[random_index], steering_ang[random_index])
     
      else:
        im = mpimg.imread(image_paths[random_index])
        steering = steering_ang[random_index]
      
      im = img_preprocess(im)
      batch_img.append(im)
      batch_steering.append(steering)
    yield (np.asarray(batch_img), np.asarray(batch_steering))  
x_train_gen, y_train_gen = next(batch_generator(X_train, y_train, 1, 1))
x_valid_gen, y_valid_gen = next(batch_generator(X_valid, y_valid, 1, 0))
 
fig, axs = plt.subplots(1, 2, figsize=(15, 10))
fig.tight_layout()
 
axs[0].imshow(x_train_gen[0])
axs[0].set_title('Training Image')
 
axs[1].imshow(x_valid_gen[0])
axs[1].set_title('Validation Image')
def nvidia_model():
  model = Sequential()
  model.add(Convolution2D(24, 5, 5, subsample=(2, 2), input_shape=(66, 200, 3), activation='elu'))
  model.add(Convolution2D(36, 5, 5, subsample=(2, 2), activation='elu'))
  model.add(Convolution2D(48, 5, 5, subsample=(2, 2), activation='elu'))
  model.add(Convolution2D(64, 3, 3, activation='elu'))
  
  model.add(Convolution2D(64, 3, 3, activation='elu'))
#   model.add(Dropout(0.5))
  
  
  model.add(Flatten())
  
  model.add(Dense(100, activation = 'elu'))
#   model.add(Dropout(0.5))
  
  model.add(Dense(50, activation = 'elu'))
#   model.add(Dropout(0.5))
  
  model.add(Dense(10, activation = 'elu'))
#   model.add(Dropout(0.5))
 
  model.add(Dense(1))
  
  optimizer = Adam(lr=1e-3)
  model.compile(loss='mse', optimizer=optimizer)
  return model
model = nvidia_model()
print(model.summary())
history = model.fit_generator(batch_generator(X_train, y_train, 100, 1),
                                  steps_per_epoch=300, 
                                  epochs=10,
                                  validation_data=batch_generator(X_valid, y_valid, 100, 0),
                                  validation_steps=200,
                                  verbose=1,
                                  shuffle = 1)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['training', 'validation'])
plt.title('Loss')
plt.xlabel('Epoch')
model.save('model.h5')
from google.colab import files
files.download('model.h5')

</pre>
</p>




</body>
</html>
